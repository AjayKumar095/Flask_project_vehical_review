{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file 'products.csv' created successfully.\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, render_template, request\n",
    "import csv\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import requests\n",
    "\n",
    "print(\"What you want to search for cars or bike:\")\n",
    "print(\"1 for car\\n2 for bikes\")\n",
    "user_input=int(input(\"Enter your choice:\"))\n",
    "\n",
    "if user_input==1:\n",
    "    car_brand=input(\"Enter the car companie name\")\n",
    "    #car_model=input(\"Enter the car model name if any or enter black space\")\n",
    "    main_url=\"https://www.zigwheels.com/\"+car_brand+\"-cars/\"\n",
    "\n",
    "elif user_input==2:\n",
    "    bike_brand=input(\"Enter the bike companie name\")\n",
    "    #bike_model=input(\"Enter the bike model name if any or enter black space\")\n",
    "    main_url=\"https://www.zigwheels.com/\"+bike_brand+\"-bikes/\"\n",
    "\n",
    "else:\n",
    "    print(None)  \n",
    "    \n",
    "response=requests.get('https://www.zigwheels.com/royal-enfield-bikes/')\n",
    "\n",
    "html_source=bs(response.text,'html.parser')\n",
    "sorted_html_page=html_source.find_all('li', {'class':\"col-lg-6 txt-c rel modelItem\"})\n",
    "\n",
    "links=[]\n",
    "for i in sorted_html_page:\n",
    "    links.append(i.find_all('a',{\"data-track-label\":\"launched-model-name\"}))\n",
    " \n",
    "filtered_links=[sublist for sublist in links if sublist]  \n",
    "    \n",
    "combined_links = list(zip(*filtered_links))\n",
    "#print(combined_links)\n",
    "html_content=[]\n",
    "for i in combined_links:\n",
    "    if type(i)==tuple:\n",
    "        for j in i:\n",
    "            html_content.append(j)\n",
    "link_list=[]            \n",
    "for i in html_content:\n",
    "   link_list.append(i['href'])\n",
    "   \n",
    "link_list\n",
    "\n",
    "#Collecting name\n",
    "model_list=[]\n",
    "for link in link_list:\n",
    "    response=requests.get(link)\n",
    "\n",
    "    html_content=bs(response.text, 'html.parser')\n",
    "    html_links=html_content.find('div',{'class':\"rel i-b mn-head model-heading-rating\"})\n",
    "\n",
    "    string_link=str(html_links)\n",
    "    string_link\n",
    "    soup=bs(string_link, 'html.parser')\n",
    "\n",
    "    model=soup.div.h1.text\n",
    "    model=model.replace('\\n','')\n",
    "    model_list.append(str(model))\n",
    "\n",
    "#collecting price\n",
    "price_list=[]\n",
    "for link in link_list:\n",
    "    response=requests.get(link)\n",
    "\n",
    "    html_content=bs(response.text, 'html.parser')\n",
    "    html_links=html_content.find('span',{'class':\"fnt-black fnt-18 b modelPrice-fnt\"})\n",
    "\n",
    "    string_link=str(html_links)\n",
    "    string_link\n",
    "    soup=bs(string_link, 'html.parser')\n",
    "\n",
    "    price=soup.span.text\n",
    "    price=price.replace('\\n','')\n",
    "    price_list.append(str(price))\n",
    "\n",
    "#collection specification\n",
    "Specifications_list_string=[]\n",
    "for link in link_list:\n",
    "      response=requests.get(link)\n",
    "\n",
    "      html_content=bs(response.text, 'html.parser')\n",
    "      html_links=html_content.find('ul',{'class':\"pb-20\"})\n",
    "      html_links=str(html_links)\n",
    "\n",
    "      soup=bs(html_links, 'html.parser')\n",
    "\n",
    "      Specifications = soup.find_all('li')\n",
    "      specifications_strings = []\n",
    "      for li in Specifications:\n",
    "          li_text = li.find(string=True, recursive=False).strip()\n",
    "          span_text = li.span.get_text(strip=True)\n",
    "          specification_string=(f\"{li_text}: {span_text}\")\n",
    "          specifications_strings.append(specification_string)\n",
    "      specifications_string_for_link = '\\n'.join(specifications_strings)\n",
    "    \n",
    "    # Print or use the specifications_string_for_link as needed\n",
    "      specifications_string_for_link=specifications_string_for_link.replace('\\n',', ')\n",
    "      Specifications_list_string.append(specifications_string_for_link)\n",
    "\n",
    "#collecting feature\n",
    "Features_string_list=[]\n",
    "for link in link_list:\n",
    "      response=requests.get(link)\n",
    "\n",
    "      html_content=bs(response.text, 'html.parser')\n",
    "      html_links=html_content.find('ul',{'id':\"keyFeatures\"})\n",
    "      html_links=str(html_links)\n",
    "\n",
    "      soup=bs(html_links,'html.parser')\n",
    "\n",
    "      Features=soup.find_all('li')\n",
    "      features_string = '\\n'.join(feature.text for feature in Features) \n",
    "      features_string=features_string.replace('\\n',', ')\n",
    "      Features_string_list.append(features_string)\n",
    "\n",
    "#collecting review\n",
    "Expert_Conclusion_string=[]\n",
    "for link in link_list:\n",
    "     response=requests.get(link)\n",
    "\n",
    "     html_content=bs(response.text, 'html.parser')\n",
    "     html_links=html_content.find('ul',{'class':\"pl-15 pt-10\"})\n",
    "     html_links=str(html_links)\n",
    "\n",
    "     soup=bs(html_links,'html.parser')\n",
    "\n",
    "     paragraph=soup.find_all('li')\n",
    "\n",
    "     paragraph=str(paragraph)\n",
    "     soup_=bs(paragraph, 'html.parser')\n",
    "\n",
    "     Expert_Conclusion=soup_.find('p',{'class':\"mb-0 clr\"})\n",
    "     Expert_Conclusion=Expert_Conclusion.text\n",
    "     Expert_Conclusion_string.append(str(Expert_Conclusion))\n",
    "\n",
    "# CSV file path\n",
    "csv_file_path = 'products.csv'\n",
    "\n",
    "# Writing to CSV file\n",
    "with open(csv_file_path, 'w', newline='') as csvfile:\n",
    "    # Create a CSV writer object\n",
    "    csv_writer = csv.writer(csvfile)\n",
    "\n",
    "    # Write the header\n",
    "    csv_writer.writerow(['name', 'price', 'specification', 'features', 'review'])\n",
    "\n",
    "    # Write data from the loops\n",
    "    for name, price, spec, features, review in zip(model_list, price_list, Specifications_list_string, Features_string_list, Expert_Conclusion_string):\n",
    "        csv_writer.writerow([ name, price, spec, features, review])\n",
    "\n",
    "print(f\"CSV file '{csv_file_path}' created successfully.\")     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Select Make']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "carnamestring=[]\n",
    "response=requests.get(\"https://www.zigwheels.com/newcars\")\n",
    "\n",
    "html_content=bs(response.text, 'html.parser')\n",
    "html_links=html_content.find('select',{'class':\"custom-select option-colors\"})\n",
    "html_links=str(html_links)\n",
    "\n",
    "soup=bs(html_links,'html.parser')\n",
    "\n",
    "paragraph=soup.find_next('option')\n",
    "paragraph=str(paragraph)\n",
    "soup_=bs(paragraph, 'html.parser')\n",
    "\n",
    "Expert_Conclusion=soup_.option.text\n",
    "\n",
    "carnamestring.append(str(Expert_Conclusion))\n",
    "carnamestring\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "carnamestring.append(str(Expert_Conclusion))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
